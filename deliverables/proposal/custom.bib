@article{zampieri_nakov_scherrer_2020, title={Natural language processing for similar languages, varieties, and dialects: A survey}, volume={26}, DOI={10.1017/S1351324920000492}, number={6}, journal={Natural Language Engineering}, publisher={Cambridge University Press}, author={Zampieri, Marcos and Nakov, Preslav and Scherrer, Yves}, year={2020}, pages={595–612}}

@inproceedings{tudoreanu-2019-dteam,
    title = "{DT}eam @ {V}ar{D}ial 2019: Ensemble based on skip-gram and triplet loss neural networks for {M}oldavian vs. {R}omanian cross-dialect topic identification",
    author = "Tudoreanu, Diana",
    booktitle = "Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",
    month = jun,
    year = "2019",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-1422",
    doi = "10.18653/v1/W19-1422",
    pages = "202--208",
    abstract = "This paper presents the solution proposed by DTeam in the VarDial 2019 Evaluation Campaign for the Moldavian vs. Romanian cross-topic identification task. The solution proposed is a Support Vector Machines (SVM) ensemble composed of a two character-level neural networks. The first network is a skip-gram classification model formed of an embedding layer, three convolutional layers and two fully-connected layers. The second network has a similar architecture, but is trained using the triplet loss function.",
}

@inproceedings{Rebeja2020ADS,
  title={A dual-encoding system for dialect classification},
  author={Petru Rebeja and Dan Cristea},
  booktitle={VARDIAL},
  year={2020}
}

@inproceedings{ceolin-2021-comparing,
    title = "Comparing the Performance of {CNN}s and Shallow Models for Language Identification",
    author = "Ceolin, Andrea",
    booktitle = "Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",
    month = apr,
    year = "2021",
    address = "Kiyv, Ukraine",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.vardial-1.12",
    pages = "102--112",
    abstract = {In this work we compare the performance of convolutional neural networks and shallow models on three out of the four language identification shared tasks proposed in the VarDial Evaluation Campaign 2021. In our experiments, convolutional neural networks and shallow models yielded comparable performance in the Romanian Dialect Identification (RDI) and the Dravidian Language Identification (DLI) shared tasks, after the training data was augmented, while an ensemble of support vector machines and Na{\"\i}ve Bayes models was the best performing model in the Uralic Language Identification (ULI) task. While the deep learning models did not achieve state-of-the-art performance at the tasks and tended to overfit the data, the ensemble method was one of two methods that beat the existing baseline for the first track of the ULI shared task.},
}

@article{DBLP:journals/corr/abs-2010-05993,
  author    = {Andrea Zugarini and
               Matteo Tiezzi and
               Marco Maggini},
  title     = {Vulgaris: Analysis of a Corpus for Middle-Age Varieties of Italian
               Language},
  journal   = {CoRR},
  volume    = {abs/2010.05993},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.05993},
  eprinttype = {arXiv},
  eprint    = {2010.05993},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-05993.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zaharia-etal-2020-exploring,
    title = "Exploring the Power of {R}omanian {BERT} for Dialect Identification",
    author = "Zaharia, George-Eduard  and
      Avram, Andrei-Marius  and
      Cercel, Dumitru-Clementin  and
      Rebedea, Traian",
    booktitle = "Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics (ICCL)",
    url = "https://aclanthology.org/2020.vardial-1.22",
    pages = "232--241",
    abstract = "Dialect identification represents a key aspect for improving a series of tasks, for example, opinion mining, considering that the location of the speaker can greatly influence the attitude towards a subject. In this work, we describe the systems developed by our team for VarDial 2020: Romanian Dialect Identification, a task specifically created for challenging participants to solve the previously mentioned issue. More specifically, we introduce a series of neural systems based on Transformers, that combine a BERT model exclusively pre-trained on the Romanian language with techniques such as adversarial training or character-level embeddings. By using these approaches, we were able to obtain a 0.6475 macro F1 score on the test dataset, thus allowing us to be ranked 5th out of 8 participant teams.",
}

@inproceeding{moroco,
  doi = {10.48550/ARXIV.1901.06543},
  url = {https://arxiv.org/abs/1901.06543},
  author = {Butnaru, Andrei M. and Ionescu, Radu Tudor},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MOROCO: The Moldavian and Romanian Dialectal Corpus},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{PolignanoEtAlCLIC2019,
  author    = {Marco Polignano and Pierpaolo Basile and Marco de Gemmis and Giovanni Semeraro and Valerio Basile},
  title     = {{AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets}},
  booktitle = {Proceedings of the Sixth Italian Conference on Computational Linguistics (CLiC-it 2019)},
  year      = {2019},
  publisher = {CEUR},
  journal={CEUR Workshop Proceedings},
  volume={2481},
  url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074851349&partnerID=40&md5=7abed946e06f76b3825ae5e294ffac14},
  document_type={Conference Paper},
  source={Scopus}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{10.5555/2999792.2999959,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

